---
title: "Final Exam2 -0698852"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE,error=FALSE,warning=FALSE)
```
## Question 2

Let us look at the dimensions to know number of rows and columns in the data
```{r}
library(rstatix)
library(nortest)
library(knitr)
nycTaxi <- read.csv("C:\\Users\\Manoja\\Downloads\\nyc-taxi.csv")
dim(nycTaxi)
```

There are 100000 observations and 16 variables in the imported data set

Following code will allow us to understand the summary of data
```{r}
summary(nycTaxi)
```
Let us look into the trip duration of this dataset.
The variable Trip duration is right skewed with very long tail. we will therefore take the log to normalise the Trip Duration distribution.


```{r}
library(ggplot2)
g <- ggplot(data=nycTaxi,aes(nycTaxi$trip_duration))
g + geom_histogram(col="blue",bins=100) + 
  labs(title="Histogram of Trip Duration")+ xlab("Trip Duration") 
```


Let us check the new plot. The log of the trip duration is normally distributed, although with a high peak.

```{r}
g <- ggplot(data=nycTaxi,aes(log(nycTaxi$trip_duration+1))) + geom_histogram(col="blue",bins=100) + 
  labs(title="Histogram of Trip Duration") + xlab("Trip Duration") 
g
```

Let us see how the tip amounts and total amounts are spread across the vendors 

```{r}
library(ggplot2)
ggplot(nycTaxi, aes(x = nycTaxi$tip_amount, y = nycTaxi$total_amount, colour = nycTaxi$Vendor)) +
  geom_point() + xlab("Tip Amount") + ylab("Total Amount") +
  facet_wrap( ~ nycTaxi$Vendor)+ labs(fill = "Vendor")
```

We can see that among the two types of vendors, VeriFone Inc has received the most tips and has earned few highest Total Amounts compared to Creative Mobile Technologies LLC.

## Section B
The independent sample t-test is the appropriate statistic to apply here. The independent sample t-test (also known as the two-sample t-test) can be used to evaluate whether or not two groups unknown population means are equal. If the data values are independent, randomly picked from two normal populations, and the two independent groups have equal variances, this test can be employed. The dataset is made up of trip duration data collected from the New York City Taxi and Limousine Commission in the morning and afternoon.

**Null hypothesis:** There is no difference between the average morning and afternoon trip durations.

**Alternate Hypothesis:** There is a difference between the average morning and afternoon trip duration.


Getting Summary statistics
```{r}
library(dplyr)
library(rstatix)
df <- filter(nycTaxi, TimeOfDay == "Morning" |TimeOfDay == "Afternoon")
time.group <- group_by(df, TimeOfDay)
get_summary_stats(time.group, trip_duration , type="mean_sd")

```
Outliers Identification

```{r}
library(knitr)
identify_outliers(time.group, trip_duration)
```
Normality and Levene Test

```{r}
library(nortest)
ad.test(filter(nycTaxi, TimeOfDay == "Morning")$trip_duration)
ad.test(filter(nycTaxi, TimeOfDay == "Afternoon")$trip_duration)
levene_test(time.group, time.group$trip_duration ~ time.group$TimeOfDay)

```
The p-value obtained from the Anderson-Darling Normality test is less than the significance level of 0.05, indicating that the distribution of differences (d) is significantly different from the normal distribution.
The nature of the variances in the two samples was determined using the Levene test. The variances for p.05 are vary according to the results of the Levene test.

T Test and cohens_d:
```{r}
t.result <- t_test(df, trip_duration ~ TimeOfDay, var.equal=FALSE)
add_significance(t.result)
```
with the asterisks, we can say p < .0001

```{r}
cohens_d(df, trip_duration ~ TimeOfDay , var.equal=FALSE)
```
we can't use p values to describe effect size, instead, we would use Cohen's d and the obtained magnitude is small.

**The current study aims to discover if there was any variation in the duration of trips that took place in the afternoon and morning. There were some outliers in the sample. The Anderson-Darling normality test revealed no group normality,  Levene's test revealed heterogeneity of variance. For 32970 samples, the mean trip duration in the Afternoon was 999.438 (sd = 829.559). For 23743 samples, the mean travel duration in the morning was 836. (sd = 672.425). According to a Welch's independent t-test, t(55926.15) = 25.89833, p 0.0001,d = 1.67, the mean difference in trip duration between morning and afternoon in the collected sample was statistically significant,  with afternoon trip tending to be longer than morning trip .**

## Confidence Interval

We want to estimate with 95% condidence the difference between the average trip durationof Afternoon and Morning.
```{r}
nycTaxi.Daygrouping <- group_by(nycTaxi, TimeOfDay)
get_summary_stats(nycTaxi.Daygrouping,trip_duration, type= "mean_sd")
```


**Afternoon: n1=32970, mean $\bar{x_{1}}$= 999.438, $s_{1}$ = 829.559**

**Morning : n2=23743, mean $\bar{x_{2}}$= 835.815, $s_{2}$ = 672.425**



95% Confidence Interval for the differenceis given by

** $x_{1}-x_{2} \pm z\times (sqrt(s^2_{1}/n_{1} + s^2_{1}/n_{2}))$ **

Where Z value at 95% is 1.96

$x_{1}-x_{2}$: is 999.559-835.815.

This means the average for Afternoon trip duration minus the average Morning trip duration, making Afternoon trip duration longer of the two time periods, in terms of this sample. Let us find if this difference is enough to generalize to the entire population with the confidence interval.

The difference between two sample means is is 163.623

```{r}
x1<- 999.438
x2<-835.815
MeanDiff<-x1-x2
MeanDiff
```
The Margin of Error is 12.38315
```{r}
s1<-829.559
s2<- 672.425
n1<- 32970
n2<-23743
a<- s1^2/n1
b<- s2^2/n2
ME<-1.96*sqrt(a+b)
ME
```
**Lower and Upper bounds at 95% CI[151.2398,176.0062 ]**
```{r}
Lower<- MeanDiff-ME
Lower
Upper <- MeanDiff+ ME
Upper
```

**Our 95 percent confidence interval for the difference between the average trip duaration in Afternoon and morning is 163.623, plus or minus 12.38315. (The lower end of the interval is 163.623- 12.38315 = 151.2398, the upper end is 163.623 + 12.38315 = 176.0062) Notice all the values in this interval are positive. That means Afternoon trip distance is estimated to be longer than Morning trip distance, based on your data.**

**To interpret these results in the context of the problem, we can say with 95 percent confidence that the Afternoon trip distance is longer, on average, than the morning trip distance, by somewhere between 151.2398 and 176.0062, based on your sample.**


## Section C

The suitable statistic to be used here is one-way ANOVA test. The one-way ANOVA ("analysis of variance") compares the means of two or more independent groups in order to determine whether there is statistical evidence that the associated population means are significantly different. 

**Null hypothesis**: There is no difference in the average trip duration between the morning, afternoon, evening, and night.

**Alternate Hypothesis**: There is difference in the average trip duration between the morning, afternoon, evening, and night

Summary
```{r}
time.grouping <- group_by(nycTaxi, TimeOfDay)
get_summary_stats(time.grouping, trip_duration, type = "mean_sd")
```

Outliers Check
```{r}
identify_outliers(time.grouping,trip_duration)
```

Normality Check
```{r}
ad.test(filter(time.grouping, TimeOfDay == "Evening")$trip_duration)
ad.test(filter(time.grouping, TimeOfDay == "Night")$trip_duration)
```

Variance Check
```{r}
levene_test(nycTaxi, trip_duration ~ TimeOfDay)
```

The means and standard deviations of the distribution were calculated. There were outliers identified in the distribution and the Anderson-Darling normality test revealed that the p-value is less than the significance level 0.05 implying that the data was not normally distributed for all the groups. The levene test revealed that the variances were heterogeneous, since p value is less than 0.05. 


ANOVA test:
```{r}
library(knitr)
kable(anova_test(nycTaxi, trip_duration ~ TimeOfDay))
```

we found significance, so we need to run a post hoc!. let's run a Tukey!

```{r}
kable(tukey_hsd(nycTaxi, trip_duration ~ TimeOfDay))

```
There was  significance identified from the output of the anova test. Since there were four groups, tukey hsd test was performed to exactly identify the groups that have shown some significance. The output of the tukeyhsd test showed p < 0.05 and p.adj.signif has '****' indicating significance for trip durations in the morning, afternoon, evening and night.


**The current study seeks to determine whether or not there were any differences in the trip durations at different times of the day. The times in a day are divided into four categories: Morning (n = 32970), Afternoon (n = 32919	), Evening (n = 23743	) and Night (n = 10368).**

**The dataset had outliers. An Anderson-Darling normality test  demonstrated no normality by group and levene's test showed heterogenity of variance. A one-way ANOVA test revealed that the trip duration were significantly different between the four groups, F(3, 99996) = 518.205, p = 0 (p < 0.05) , $\eta^2$ = 0.015.**

**The trip duration in the afternoon (M=999.438, SD=829.559) was higher than the trip duration in the evening (M=868., SD=624.), morning (M=835.815, SD=672.425) and night (M=725.351, SD=519.064).**

**Tukey post-hoc analysis revealed that the differences of the trip duration exists between Afternoon, evening(-131.93295, 95% CI[-145.94824, -117.91765]), Afternoon Morning(-163.62393, 95% CI[ -178.93455, -148.31332]),  Afternoon Night (-274.08702, 95% CI[-294.34080, -253.83325]), Evening Morning(-31.69099 95% CI[-47.00657, -16.37541]), Evening night (-142.15408 95% CI[-162.41161, -121.89655]), and Morning Night (-110.46309 95% CI[-131.63746, -89.28872]) with p < .05. Hence , the null hypothesis was rejected.**

## Section D
The suitable statistic to be used here is two-way ANOVA test. The two-way ANOVA is used when we want to know how two independent variables, in combination, affect a dependent variable.

Summary statistics
```{r}
library(dplyr)
library(rstatix)
grouping<- group_by(nycTaxi, TimeOfDay, PUborough)
get_summary_stats(grouping, trip_duration, type = "mean_sd")
```
Outliers Check
```{r}
identify_outliers(grouping,trip_duration)
```

Normality Check
```{r}
ad.test(filter(grouping, TimeOfDay == "Morning")$trip_duration)
ad.test(filter(grouping, TimeOfDay == "Afternoon")$trip_duration)
ad.test(filter(grouping, TimeOfDay == "Evening")$trip_duration)
ad.test(filter(grouping, TimeOfDay == "Night")$trip_duration)
```

Variance Check
```{r}
levene_test(nycTaxi, trip_duration ~ TimeOfDay * PUborough)
```
The means and standard deviations of the distribution were calculated. There were outliers identified in the distribution and the Anderson-Darling normality test revealed that the p-value is less than the significance level 0.05 implying that the data was not normally distributed for all the groups. The levene test revealed that the variances were heterogeneous, since p value is less than 0.05.

let's run Anova test
```{r}
anova_test(nycTaxi, trip_duration ~ TimeOfDay * PUborough)
```
We found significance let us run post hoc!

```{r, fig.width= 10, fit.height = 10}
library(knitr)
model <- lm(trip_duration ~ TimeOfDay * PUborough, nycTaxi)

 with(nycTaxi, interaction.plot(x.factor = nycTaxi$PUborough, trace.factor = nycTaxi$TimeOfDay, 
                                 response = nycTaxi$trip_duration, type = "b",
                                 ylab = "Trip Duration", xlab = "PUBorough", lty = 1, lwd=3, 
                                 col = c("blue", "purple", "red", "dark green"), trace.label = "Time of Day"))
time.grouping <- group_by(nycTaxi, TimeOfDay)
anova_test(time.grouping, trip_duration ~ PUborough, error = model)
```



There is a significance difference among all. Let us see where the differences lie. Let us run pariwise comparison.
```{r}
library(knitr)
library(emmeans)
emmeans_test(time.grouping, trip_duration ~ PUborough, p.adjust.method = "bonferroni")

```
This gives all possible combinations and helps in finding the significant ones.


**The current study seeks to determine wether or not there an interaction between PUborough and TimeOfDay when considering average trip duration. The times in a day are divided into four categories: Morning (n = 32970), Afternoon (n = 32919	), Evening (n = 23743	) and Night (n = 10368).**

**The dataset had outliers. An Anderson Darling test demonstrated no normality by group and levene's test showed heterogenity of variance. The ANOVA table indicates that the interaction effect is significant,with p < .05.Hence we reject null hypothesis. An interracted plot was plotted for the available data. All four lines have shown increase for Bronx, Brooklynn, EWR, Manhattan, Queens and Staten Island. Further analysis of main effect for "PUBorough" was performed with statistical significance receiving a tukey adjustment. During EVENING TIME and when the pick up point is from STATEN ISLAND, passenger can expect longest trip duration **

**According to ANOVA test there was a  statistically significant interaction effect between the TimeOfDay and PUborough, F(11, 99980) = 149.294, $\eta^2$ = 0.016, p < 0.05. Consequently, an analysis of main effects  was performed with statistical significance receiving  a Bonferroni adjustment. There is a significance difference between Afternoon, PUborough F(4,99980)=2759. p<0.05, Evening, PUborough F(4,99980)=979. p<0.05, Morning, PUborough F(5,99980)=990. p<0.05, Night, PUborough F(3,99980)=72.2, p<0.05.**

**All pairwise comparisions were analysed between the different levels, there were significant difference between PUborough and TimeOfDay when considering average trip duration.**





## Section E

During Afternoon Passengers tend to spend more time on an average of 999.438.
```{r}
nycTaxi.Daygrouping <- group_by(nycTaxi, TimeOfDay)
get_summary_stats(nycTaxi.Daygrouping,trip_duration, type= "mean_sd")
library(ggplot2)
ggplot(nycTaxi, aes(x = nycTaxi$trip_duration, y = nycTaxi$total_amount, colour = nycTaxi$TimeOfDay)) +geom_point() + xlab("Trip time") + ylab("Total Amount") +
  facet_wrap( ~ nycTaxi$TimeOfDay)+ labs(fill = "TimeOfDay")
```
The graph also illustrates that the Afternoon trip time is more distributed than the rest of the day time.

## Question 3

Any regression or correlation studies generally assume the following assumptions:

* Linearity of the data. The relationship between the predictor (x) and the outcome (y) is assumed to be linear.

* Normality of residuals. The residual errors are assumed to be normally distributed.

* Homogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity).

* Independence of residuals error terms

```{r}
lmmodel<- lm(nycTaxi$tip_amount ~., data = nycTaxi)
par(mfrow = c(2,2))
plot(lmmodel)

```

The diagnostic plots show residuals in four different ways:

* Residuals vs Fitted: Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship. We have many points outside the linear relationship which is violating linearity.

* Normal Q-Q: Used to examine whether the residuals are normally distributed. It is good if residuals points follow the straight dashed line. However we have observations outside the straight line.

* Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. This is not the case in our example, where we have a heteroscedasticity proble.

* Residuals vs Leverage. Used to identify influential cases, that is extreme values which are many in the plot, that might influence the regression results when included or excluded from the analysis.

The four plots show the top  most extreme data points labeled with with the row numbers of the data in the data set. They might be potentially problematic. we can take a close look at them individually to check if there is anything special for the subject or if it could be simply data entry errors.


The following plot illustrate the Cook's distance

```{r}
plot(lmmodel,4)
```

By default, the top 3 most extreme values are labelled on the Cook's distance plot. i.e 24730, 36979, 92456.


Let's interpret linear regression model:

```{r}
summary(lmmodel)
```
The above summary shows that the 14 predictor variables  showed with alpha level less than 0.05. Hence, these predictor variables were statistically significant.The residual standard error was 0.1035 on 99969 degrees of freedom. This value is expected to be small for the regression model to be able to fit the data better.Multiple R-Squared was recorded as 0.9982. The closer this value is to 1, the better the predictor variables are able to predict the value of the response variable.The adjusted R-squared was 0.9982 which can be used for comparing the fit of different regression models that use different numbers of predictor variables. This Linear regression model seems to work well as the  model estimates yielded good values i.e.R-square, adj R-square,erro values etc. We can not find out which regression model is the best of all unless all are implemented and compared. Since the data has lot of outliers and noise let us look into lasso model as it is not sensitive to outliers or noise unlike linear regression. When we have a high dimensional dataset, it would be highly inefficient to use all the variables since some of them might be imparting redundant information. We would need to select the right set of variables which give us an accurate model as well as able to explain the dependent variable well. Though Lasso's prediction performance is quite poor, it selects features by shrinking co-efficient towards zero which is absent in case of other regressio models. 


To perform lasso regression, we will use functions from the glmnet package. This package requires the response variable to be a vector and the set of predictor variables to be of the class data.matrix. we'll use the glmnet() function to fit the lasso regression model and specify alpha=1. Setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net. 

```{r}
library(glmnet)
library(Rcpp)
## Lasso
# response variable
y <- nycTaxi$tip_amount
# matrix with predictor variables
X <- model.matrix(~., nycTaxi[, -which(names(nycTaxi) == "tip_amount")])[, -1]
cv_model <- cv.glmnet(X, y, alpha = 1)
cv_model
```
To determine what value to use for lambda, we'll perform k-fold cross-validation and identify the lambda value that produces the lowest test mean squared error (MSE).Note that the function cv.glmnet() automatically performs k-fold cross validation using k = 10 folds.

```{r}
best_lambda <- cv_model$lambda.min
best_lambda

```

The lambda value that minimizes the test MSE turns out to be 0.0008299607.

Lastly, we can analyze the final model produced by the optimal lambda value.
```{r}
lasso_model <- glmnet(X, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)

```

For predicters with no coefficient, lasso regression has shrunk them all the way to zero. This means it was completely dropped from the model because they were not influential enough. 

When looking at the estimates, positive estimated variables like the pick-up point in Queens, the drop-off point in Manhattan, credit card payment, and lastly the total amount(higher estimate of all), made an impact on the tips amount. So, in order for the taxi driver to make more tips, he should concentrate more on picking up customers from Queens, customers travelling to Manhattan, customers who like to pay with a credit card, and finally, if he can earn a higher total amount. This is only possible if the trip distance or duration is long, resulting in a higher total amount of money. This maximization of tips are subjective to this dataset.

we shall set up the data frame to show the raw correlations in a table. The highest correlation is sorted first in the data set. Only variables above a certain significance level threshold are selected in order to limit the sheer number of variables (without having to manually pick and choose). As a starting point, it's set at 0.5. After the table is created, it will return the correlation matrix chart below, which has been filtered out. Only correlations with a significant enough level of significance will have a coloured circle. If there are still a lot of variables, this helps to cut out the noise even further. After some feature engineering, the 'corr simple' function can be used again and again, or with varying significance levels.data quickly and to see only the information that is relevant. With more variables, it may be essential to experiment with alternative significance thresholds and/or employ additional feature engineering to limit the number of associated variables before re-running the function until the results are understandable and helpful.The correlation plot also shows the impact of total_amount on tip_amount

 

```{r}
library(dplyr)
library(corrplot)
corr_simple <- function(data=nycTaxi,sig=0.5){
  #convert data to numeric in order to run correlations
  #convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
  df_cor <- data %>% mutate_if(is.character, as.factor)
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
corr_simple()
```




## Question 4

* a) Do tax riders tip differently in Bronx and Brooklyn?

* b) The columns that would be needed are PUborough, DOborough, tip_amount.

* c) * Null Hypothesis : Same tip amounts in both boroughss.

        $\mu_{1}$ = $\mu_{2}$

     * Alternate Hypothesis: Different tip amounts in both boroughs.
     
       $\mu_{1}$ $\not=$ $\mu_{2}$

* d) Test : Welch's t-test can be used as it tests for the difference between the means of two populations, based on two independent samples.

Bronx to Bronx : n1=70, mean $\bar{x_{1}}$= 0.297, $s_{1}$ = 0.940

Brooklyn to Brooklyn : n2=853, mean $\bar{x_{2}}$= 1.07, $s_{2}$ = 1.69
```{r}
library(dplyr)
nycTaxi%>% group_by(PUborough, DOborough) %>%
  summarise(mean(tip_amount), sd(tip_amount))
```
After Calculating we get a t-score of -6.117 with a p value 0.00000002. We can reject null hypothesis at the significance level 0.05, because p-value does not exceed 0.05. Hence, we can conclude that the average tip amounts are different at Bronx and Brooklyn. 






## Question 1 Answers
Logistic regression model with all 7 explanatory variables npreg, glu, bp, skin, bmi, ped, age. Model 1 summary

```{r}
library(MASS)

itrain=c(7, 16, 96, 160, 9, 28, 1, 27, 178, 14, 121, 111, 45, 143, 93, 172, 200, 126, 100, 44, 5, 164, 22, 54, 151, 117, 124, 17, 191, 114, 175, 15, 152, 43, 189, 84, 192, 169, 49, 190, 137, 72, 79, 133, 34, 105, 186, 102, 182, 98, 142, 13, 58, 155, 166, 165, 179, 12, 92, 162, 199, 135, 41, 138, 194, 110, 62, 30, 145, 82, 71, 197, 86, 103, 193, 81, 42, 18, 55, 24, 11, 25, 171, 47, 35, 106, 174, 184, 70, 112, 74, 73, 36, 115, 104, 90, 153, 154, 94, 64, 19, 108, 157, 97, 99, 10, 60, 4, 80, 140, 173, 161, 149, 76, 26, 37, 52, 66, 129, 85, 167, 127, 8, 187, 119, 32, 176, 61, 150, 77, 183, 120, 2, 144, 59, 20, 75, 139, 68, 50, 118, 39, 116, 3, 91, 141, 31, 195, 128, 101, 63, 130, 163, 159, 95, 196, 53, 57, 88, 134, 56, 21, 48, 125, 69, 136, 65, 33, 177, 188)

ihold=c(91, 212, 265, 255, 132, 87, 304, 232, 90, 181, 262, 58, 197, 312, 115, 206, 116, 310, 291, 113, 166, 194, 293, 161, 281, 188, 1, 119, 5, 31, 208, 221, 76, 286, 99, 263, 226, 38, 8, 110, 77, 163, 184, 259, 235, 186, 124, 306, 185, 327, 315, 68, 307, 62, 211, 82, 245, 16, 94, 69, 100, 59, 278, 19, 45, 175, 270, 264, 107, 178, 4, 317, 331, 28, 66, 233, 106, 316, 56, 30, 131, 33, 149, 55, 284, 130, 134, 104, 86, 205, 198, 155, 169, 141, 70, 35, 267, 49, 103, 240, 61, 79, 180, 228, 248, 196, 237, 231, 57, 26, 117, 332, 203, 23, 114, 170, 158, 322, 294, 36, 34, 11, 20, 172, 256, 320, 95, 135, 292, 182, 302, 154, 309, 290, 18, 216, 63, 269, 319, 280, 238, 105, 210, 159, 102, 174, 308, 25, 201, 247, 250, 301, 72, 145, 329, 251, 142, 15, 167, 153, 140, 146, 234, 78, 17, 138, 289, 252, 313, 261, 193, 148, 7, 189, 52, 120, 202, 271, 73, 215, 151, 29, 122, 190, 164, 177, 13, 80, 46, 314, 14, 298, 283, 236, 223, 258, 89, 191, 101, 71)

data(Pima.tr)
mytrain=Pima.tr[itrain,]
data(Pima.te)
myhold=Pima.te[ihold,]

mytrain$type<-ifelse(mytrain$type=="Yes",1,0)
myhold$type<-ifelse(myhold$type=="Yes",1,0)

# 1)logistic regression model with all 7 explanatory variables npreg, glu, bp, skin, bmi, ped, age. Called this model 1.
model_1 <- glm(type ~. , family = binomial(link = "logit"), data = mytrain)
summary(model_1)
```

**Part a) Model 1 regression coefficients, Coefficient for ped is:1.405025283**
```{r}
model_1$coefficients
```
The logistic regression model with 4 explanatory variables glu, bmi, ped, age (this is best model from backward elimination if all cases of Pima.tr is used). For this model with 4 explanatory variables, called it model 2

```{r}
model_2 <- glm(type ~ glu+bmi+ped+age, family = binomial(link = "logit"), data =mytrain)
summary(model_2)
```

**Part b) Model 2 regression coefficients, Coefficient for age is:0.05266934**
```{r}
model_2$coefficients
```

**Part c) Model 1: The predicted Probability for first subject 91 is 0.19881247** 
```{r}
library(dplyr)
#Fitting Myhold dataset and it's probability
probabilities_1 <- model_1 %>% predict(myhold, type = "response")
#Subject probabilities
head(probabilities_1)
```

**Model 2: The predicted Probability for first subject 91 is 0.1720737**
```{r}
#Fitting Myhold dataset and it's probability
probabilities_2 <- model_2 %>% predict(myhold, type = "response")
#Subject probabilities
head(probabilities_2)
```

**Part d) Model 1: Boundary of 0.5 in the predicted probabilities to decide on diabetes (predicted probability greater than or equal to 0.5) or non-diabetes. The total number of misclassifications of the 200 cases in the holdout set is 39.**

```{r}
predicted.classes_1 <- ifelse(probabilities_1 >= 0.5,1,0)
#Total missclassified cases
sum(predicted.classes_1!= myhold$type)



```
**Model 2: Boundary of 0.5 in the predicted probabilities to decide on diabetes (predicted probability greater than or equal to 0.5) or non-diabetes. The total number of misclassifications of the 200 cases in the holdout set is 39.**

```{r}
predicted.classes_2 <- ifelse(probabilities_2 > 0.5,1,0)
#Total missclassified cases
sum(predicted.classes_2!= myhold$type)


```

**Part e) Model 1 Misclassification rate : 0.195**

```{r}
#Model 1 Missclassification Accuracy
mean(predicted.classes_1!= myhold$type)
```

**Model 2 Misclassification rate : 0.195**
```{r}
#Model 2 Missclassification Accuracy
mean(predicted.classes_2!= myhold$type)
```
 **Part f) Model 1: Boundary of 0.3 in the predicted probabilities to decide on diabetes (predicted probability greater than or equal to 0.3) or non-diabetes. The total number of misclassifications of the 200 cases in the holdout set is 45.**
 
```{r}
#With 0.3 boundary
predicted.classes_b3 <- ifelse(probabilities_1 >= 0.3,1,0)
sum(predicted.classes_b3!= myhold$type)
```
 
**Model 2: Boundary of 0.3 in the predicted probabilities to decide on diabetes (predicted probability greater than or equal to 0.3) or non-diabetes. The total number of misclassifications of the 200 cases in the holdout set is 40.**

```{r}
# with 0.3 boundary
predicted.classes_b3 <- ifelse(probabilities_2 >= 0.3,1,0)
sum(predicted.classes_b3!= myhold$type)
```





```{r, echo=FALSE, results=FALSE,message= FALSE}
#FULL CODE
library(MASS)

itrain=c(7, 16, 96, 160, 9, 28, 1, 27, 178, 14, 121, 111, 45, 143, 93, 172, 200, 126, 100, 44, 5, 164, 22, 54, 151, 117, 124, 17, 191, 114, 175, 15, 152, 43, 189, 84, 192, 169, 49, 190, 137, 72, 79, 133, 34, 105, 186, 102, 182, 98, 142, 13, 58, 155, 166, 165, 179, 12, 92, 162, 199, 135, 41, 138, 194, 110, 62, 30, 145, 82, 71, 197, 86, 103, 193, 81, 42, 18, 55, 24, 11, 25, 171, 47, 35, 106, 174, 184, 70, 112, 74, 73, 36, 115, 104, 90, 153, 154, 94, 64, 19, 108, 157, 97, 99, 10, 60, 4, 80, 140, 173, 161, 149, 76, 26, 37, 52, 66, 129, 85, 167, 127, 8, 187, 119, 32, 176, 61, 150, 77, 183, 120, 2, 144, 59, 20, 75, 139, 68, 50, 118, 39, 116, 3, 91, 141, 31, 195, 128, 101, 63, 130, 163, 159, 95, 196, 53, 57, 88, 134, 56, 21, 48, 125, 69, 136, 65, 33, 177, 188)

ihold=c(91, 212, 265, 255, 132, 87, 304, 232, 90, 181, 262, 58, 197, 312, 115, 206, 116, 310, 291, 113, 166, 194, 293, 161, 281, 188, 1, 119, 5, 31, 208, 221, 76, 286, 99, 263, 226, 38, 8, 110, 77, 163, 184, 259, 235, 186, 124, 306, 185, 327, 315, 68, 307, 62, 211, 82, 245, 16, 94, 69, 100, 59, 278, 19, 45, 175, 270, 264, 107, 178, 4, 317, 331, 28, 66, 233, 106, 316, 56, 30, 131, 33, 149, 55, 284, 130, 134, 104, 86, 205, 198, 155, 169, 141, 70, 35, 267, 49, 103, 240, 61, 79, 180, 228, 248, 196, 237, 231, 57, 26, 117, 332, 203, 23, 114, 170, 158, 322, 294, 36, 34, 11, 20, 172, 256, 320, 95, 135, 292, 182, 302, 154, 309, 290, 18, 216, 63, 269, 319, 280, 238, 105, 210, 159, 102, 174, 308, 25, 201, 247, 250, 301, 72, 145, 329, 251, 142, 15, 167, 153, 140, 146, 234, 78, 17, 138, 289, 252, 313, 261, 193, 148, 7, 189, 52, 120, 202, 271, 73, 215, 151, 29, 122, 190, 164, 177, 13, 80, 46, 314, 14, 298, 283, 236, 223, 258, 89, 191, 101, 71)

data(Pima.tr)
mytrain=Pima.tr[itrain,]
data(Pima.te)
myhold=Pima.te[ihold,]

mytrain$type<-ifelse(mytrain$type=="Yes",1,0)
myhold$type<-ifelse(myhold$type=="Yes",1,0)

#MODEL 1
model_1 <- glm(type ~. , family = binomial(link = "logit"), data = mytrain)
summary(model_1)
model_1$coefficients
#Fitting Myhold dataset and it's probability
probabilities_1 <- model_1 %>% predict(myhold, type = "response")
#Subject probabilities
head(probabilities_1)
predicted.classes_1 <- ifelse(probabilities_1 >= 0.5,1,0)
head(predicted.classes_1)
# Model accuracy
mean(predicted.classes_1 == myhold$type)
#Model Missclassification Accuracy
mean(predicted.classes_1!= myhold$type)
sum(predicted.classes_1!= myhold$type)



#Probability with 0.3 boundary
predicted.classes_b3 <- ifelse(probabilities_1 >= 0.3,1,0)
head(predicted.classes_b3)
# Model accuracy
mean(predicted.classes_b3 == myhold$type)
#Model Missclassification Accuracy
mean(predicted.classes_b3!= myhold$type)
#Total missclassified cases
sum(predicted.classes_b3!= myhold$type)



#MODEL 2
model_2 <- glm(type ~ glu+bmi+ped+age, family = binomial(link = "logit"), data =mytrain)
summary(model_2)
model_2$coefficients
#Fitting Myhold dataset and it's probability
probabilities_2 <- model_2 %>% predict(myhold, type = "response")
#Subject probabilities
head(probabilities_2)
# Model accuracy
mean(predicted.classes_2 == myhold$type)
#Model Missclassification Accuracy
mean(predicted.classes_2!= myhold$type)
#Total missclassified cases
sum(predicted.classes_2!= myhold$type)


#Probability with 0.3 boundary
predicted.classes_b3 <- ifelse(probabilities_2 >= 0.3,1,0)
head(predicted.classes_b3)
# Model accuracy
mean(predicted.classes_b3 == myhold$type)
#Model Missclassification Accuracy
mean(predicted.classes_b3!= myhold$type)
sum(predicted.classes_b3!= myhold$type)


```

